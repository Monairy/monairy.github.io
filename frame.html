<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>Source.frame API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Source.frame</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
from Source.Loss import *
from Source.activations import *
from Source.evaluation import *
from Source.optmizers import *
from Source.data import *




class MultiLayer:
    def __init__(self, number_of_neurons=0, cost_func=cross_entropy):
        &#34;&#34;&#34; init of the class multilayer and needed variables
        variables:
            w,b lists for weights
            parameters dic for weights in the form of parameters[&#39;W1&#39;]
            layers_size for size of each layer
            number_of_input_neurons
            act_func list for activations of each layer
            derivative_act_func list for backward activations derivative functions
            cost_func the choosen cost functions

        parmeters:
            (method) : the cost function of model

        returns:
            (None)

        &#34;&#34;&#34;
        self.w, self.b = [], []
        self.parameters = {}
        self.layer_size = []

        self.number_of_input_neurons = number_of_neurons
        self.number_of_outputs = 0

        self.act_func = []
        self.derivative_act_func = []

        self.cost_func = cost_func
        self.cost_func_der = determine_der_cost_func(self.cost_func)

        self.cache = {}
        self.prev = []

    def addLayerInput(self, size):
        &#34;&#34;&#34; add the input layer of the model

        parmeters:
            size (int) : size of input layer

        retruns:
            (None)

        &#34;&#34;&#34;
        self.number_of_input_neurons = size
        self.layer_size.append(size)

    def addHidenLayer(self, size, act_func=sigmoid):
        &#34;&#34;&#34; add a hidden layer of the model

        parmeters:
            size (int) : size of input layer
            act_func (function) : the activation function of the layer
        
        retruns:
            (None)
        &#34;&#34;&#34;
        self.layer_size.append(size)
        self.act_func.append(act_func)
        self.derivative_act_func.append(determine_der_act_func(act_func))

    def addOutputLayer(self, size, act_func=sigmoid):
        &#34;&#34;&#34; add the output layer of the model

        parmeters:
            size (int) : size of input layer
            act_func (function) : the activation function of the layer
        
        retruns:
            (None)

        &#34;&#34;&#34;
        self.number_of_outputs = size
        self.layer_size.append(size)
        self.act_func.append(act_func)
        self.derivative_act_func.append(determine_der_act_func(act_func))

    def initialize_parameters(self, seed=2): #,init_func=random_init_zero_bias):
        &#34;&#34;&#34; initialize_weights of the model at the start with xavier init

        parmeters:
            seed (int) : seed for random function

        retruns:
            paramters

        &#34;&#34;&#34;

        # todo very important check later

        np.random.seed(seed)  # we set up a seed so that your output matches ours although the initialization is random.

        L = len(self.layer_size)  # number of layers in the network

        for l in range(1, L):
            self.w.append(np.random.randn(self.layer_size[l], self.layer_size[l - 1]) * np.sqrt
            (2 / self.layer_size[l - 1]))  # *0.01
            self.b.append(np.zeros((self.layer_size[l], 1)))
            # seed += 1
            # np.random.seed(seed)

        for i in range(len(self.layer_size) - 1):
            self.parameters[&#34;W&#34; + str(i + 1)] = self.w[i]
            self.parameters[&#34;b&#34; + str(i + 1)] = self.b[i]

        return self.parameters

    def forward_propagation(self, X, drop=0):
        &#34;&#34;&#34; forward propagation through the layers

        parmeters:
            X (np.array) : input feature vector
            drop (float) : propablity to keep neurons or shut down
       
        retruns:
            cashe (dic) : the output of each layer in the form of cashe[&#39;Z1&#39;]
            Alast (np.array) : last layer activations


        &#34;&#34;&#34;

        self.prev = []
        self.prev.append((1, X))
        for i in range(len(self.layer_size) - 1):
            Zi = np.dot(self.w[i], self.prev[i][1]) + self.b[i]
            Ai = self.act_func[i](Zi)
            if drop &gt; 0 and i != len(self.layer_size) - 2:
                D = np.random.rand(Ai.shape[0], Ai.shape[1])
                D = D &lt; drop
                Ai = Ai * D
                Ai = Ai / drop

            self.prev.append((Zi, Ai))

        A_last = self.prev[-1][1]

        for i in range(len(self.layer_size) - 1):
            self.cache[&#34;Z&#34; + str(i + 1)] = self.prev[i + 1][0]
            self.cache[&#34;A&#34; + str(i + 1)] = self.prev[i + 1][1]

        # todo sould i compute cost in here

        return A_last, self.cache

    def set_cost(self, cost_func):
        &#34;&#34;&#34; cahnge the initial cost function

        parmeters:
            cost_funct (function) : the new function
        
        retruns:
            cashe (dic) : the output of each layer in the form of cashe[&#39;Z1&#39;]
            Alast (np.array) : last layer activations

        &#34;&#34;&#34;
        self.cost_func = cost_func
        self.cost_func_der = determine_der_cost_func(cost_func)

    def compute_cost(self, Alast, Y):
        &#34;&#34;&#34; compute cost of the given examples

        parmeters:
            Alast (np.array) : model predictions
            Y (np.array) : True labels
        
        retruns:
            cost (float) : cost output

        &#34;&#34;&#34;
        m = Alast.shape[1]
        return self.cost_func(m, Alast, Y)

    def backward_propagation(self, X, Y):
        &#34;&#34;&#34; compute cost of the given examples

        parmeters:
            Alast (np.array) : model predictions
            Y (np.array) : True labels
        
        retruns:
            grads (dic) : all gridients of wieghts and biasses

        &#34;&#34;&#34;

        m = X.shape[1]

        # todo all depends on the type of function in cost and actviation function
        grad_list1_w = []
        grad_list1_b = []

        Alast = self.prev[-1][1]
        final_act = self.derivative_act_func[-1]
        dzi = self.cost_func_der(m, Alast, Y) * final_act(Alast)

        if self.cost_func == cross_entropy:
            if self.act_func[-1] == sigmoid:
                pass

        for i in range(len(self.w), 0, -1):
            A = self.prev[i - 1][1]
            dwi = (1 / m) * np.dot(dzi, self.prev[i - 1][1].T)
            dbi = (1 / m) * np.sum(dzi, axis=1, keepdims=True)
            if i != 1:
                der_func = self.derivative_act_func[i - 2]
                A = self.prev[i - 1][1]
                dzi = np.multiply(np.dot((self.w[i - 1]).T, dzi), der_func(A))

            grad_list1_w.append(dwi)
            grad_list1_b.append(dbi)

        # reverse grad list
        grad_list_w = []
        grad_list_b = []

        for i in range(len(grad_list1_w) - 1, -1, -1):
            grad_list_w.append(grad_list1_w[i])
            grad_list_b.append(grad_list1_b[i])

        grads = {}

        for i in range(len(grad_list_w)):
            grads[&#39;dW&#39; + str(i + 1)] = grad_list_w[i]
            grads[&#39;db&#39; + str(i + 1)] = grad_list_b[i]

        return grads

    def set_cashe(self, cache, X):
        &#34;&#34;&#34; set an external cache

        parmeters:
            X (np.array) : input feature vector
            cache (dic) :  output of each layer
        
        retruns:
            (None)

        &#34;&#34;&#34;
        self.cache = cache
        self.prev = []
        self.prev.append((1, X))
        for i in range(int(len(cache.keys()) / 2)):
            A, Z = cache[&#34;A&#34; + str(i + 1)], cache[&#34;Z&#34; + str(i + 1)]
            self.prev.append((Z, A))

    def set_parameters(self, para):
        &#34;&#34;&#34; set an external parmeters

        parmeters:
            para (dic) :  the weights and biasses
        
        retruns:
            (None)

        &#34;&#34;&#34;
        self.parameters = para
        self.w = []
        self.b = []
        for i in range(int(len(para.keys()) / 2)):
            W, b = para[&#34;W&#34; + str(i + 1)], para[&#34;b&#34; + str(i + 1)]
            self.w.append(W)
            self.b.append(b)

    def set_parameters_internal(self):
        &#34;&#34;&#34; set an internal parmeters this is used by model during training

        parmeters:
            (None)
        
        retruns:
            (None)

        &#34;&#34;&#34;
        self.parameters = {}
        for i in range(len(self.w)):
            self.parameters[&#34;W&#34; + str(i + 1)] = self.w[i]
            self.parameters[&#34;b&#34; + str(i + 1)] = self.b[i]

    def update_parameters(self, grads, learning_rate=1.2 , reg_term=0, m = 1):
        &#34;&#34;&#34; update parameters using grads

        parmeters:
            grads (dic) :  the gradient of weights and biases
            learning_rate (float) : the learn rate hyper parameter
            reg_term (float) : the learn rate hyper parameter
        
        returns:
            dictionary contains the updated parameters

        &#34;&#34;&#34;

        for i in range(len(self.w)):
            self.w[i] = (1-reg_term/m) * self.w[i] - learning_rate * grads[&#34;dW&#34; + str(i + 1)]
            self.b[i] = (1-reg_term/m) * self.b[i] - learning_rate * grads[&#34;db&#34; + str(i + 1)]

        self.set_parameters_internal()

        return self.parameters

    def update_parameters_adagrad(self, grads,adagrads, learning_rate=1.2, reg_term=0, m = 1):
        &#34;&#34;&#34; update parameters using adagrad

        parameters:
            grads (dic) :  the gradient of weights and biases
            adagrads(dic): the square of the gradiant
            learning_rate (float) : the learn rate hyper parameter
            reg_term (float) : the learn rate hyper parameter
        
        returns:
            dictionary contains the updated parameters

        &#34;&#34;&#34;

        for i in range(len(self.w)):

            self.w[i] = (1-reg_term/m) * self.w[i] - (learning_rate / (np.sqrt(adagrads[&#34;dW&#34; + str(i + 1)]) + 0.000000001)) * grads[&#34;dW&#34; + str(i + 1)]
            self.b[i] = (1-reg_term/m) * self.b[i] - (learning_rate / (np.sqrt(adagrads[&#34;db&#34;+str(i+1)]) + 0.000000001)) * grads[&#34;db&#34; + str(i + 1)]
        self.set_parameters_internal()

        return self.parameters

    def upadte_patameters_RMS(self, grads,rmsgrads, learning_rate=1.2 , reg_term=0, m = 1,eps=None):
        &#34;&#34;&#34; update parameters using RMS gradient

        parameters:
            grads (dic) :  the gradient of weights and biases
            rmsgrads(dic): taking rho multiplied by the square of previous grads and (1-rho) multiplied by the square of current grads
            learning_rate (float) : the learn rate hyper parameter
            reg_term (float) : the learn rate hyper parameter
            eps(float) : the small value added to rmsgrads to make sure there is no division by zero
       
        returns:
            dictionary contains the updated parameters

        &#34;&#34;&#34;

        for i in range(len(self.w)):

            self.w[i] = (1-reg_term/m) * self.w[i] - (learning_rate / (np.sqrt(rmsgrads[&#34;dW&#34; + str(i + 1)]) + eps)) * grads[&#34;dW&#34; + str(i + 1)]
            self.b[i] = (1-reg_term/m) * self.b[i] - (learning_rate / (np.sqrt(rmsgrads[&#34;db&#34;+str(i+1)]) + eps)) * grads[&#34;db&#34; + str(i + 1)]
        self.set_parameters_internal()

        return self.parameters

    def upadte_patameters_adadelta(self, grads,delta, learning_rate=1.2, reg_term=0, m = 1):
        &#34;&#34;&#34; update parameters using RMS gradient

        parameters:
            grads (dic) :  the gradient of weights and biases, note: this parameter is not used in this function
            delta(dic): dictionary contains the values that should be subtracted from current parameters to be updated
            learning_rate (float) : the learn rate hyper parameter , note: this parameter is not used in this function
            reg_term (float) : the learn rate hyper parameter
       
        returns:
            dictionary contains the updated parameters

        &#34;&#34;&#34;


        for i in range(len(self.w)):

            self.w[i] = (1-reg_term/m) * self.w[i] - delta[&#34;dW&#34; + str(i + 1)]
            self.b[i] = (1-reg_term/m) * self.b[i] - delta[&#34;db&#34; + str(i + 1)]
        self.set_parameters_internal()

        return self.parameters

    def update_parameters_adam(self, grads,adamgrads,Fgrads, learning_rate=1.2, reg_term=0, m = 1,eps=None):
        &#34;&#34;&#34; update parameters using RMS gradient

        parameters:
            grads (dic) :  the gradient of weights and biases , note: grads is not used in this function
            adamgrads(dic): taking rho multiplied by the square of previous grads and (1-rho) multiplied by the square of current grads
            Fgrads(dic): taking rhof multiplied by the  previous grads and (1-rhof) multiplied by the  current grads
            learning_rate (float) : the learn rate hyper parameter (alpha_t not alpha)
            reg_term (float) : the learn rate hyper parameter
            eps(float) : the small value added to adamgrads to make sure there is no division by zero
        
        returns:
            dictionary contains the updated parameters

        &#34;&#34;&#34;

        for i in range(len(self.w)):

            self.w[i] = (1-reg_term/m) * self.w[i] - (learning_rate/np.sqrt(adamgrads[&#34;dW&#34;+str(i+1)] + eps)) * Fgrads[&#34;dW&#34; + str(i + 1)]
            self.b[i] = (1-reg_term/m) * self.b[i] - (learning_rate /np.sqrt(adamgrads[&#34;db&#34;+str(i+1)] + eps)) * Fgrads[&#34;db&#34; + str(i + 1)]
        self.set_parameters_internal()

        return self.parameters

    def train(self, X, Y, num_iterations=10000, print_cost=False , print_cost_each=100, cont=0, learning_rate=1 , reg_term=0 , batch_size=0 , opt_func=gd_optm, param_dic=None,drop=0):
        &#34;&#34;&#34; train giving the data and hpyerparmeters and optmizer type
        
        parmeters:
            X (np.array) : input feature vector
            Y (np.array) :  the true label
            num_of iterations (int) : how many epochs
            print cost (bool) : to print cost or not
            print cost_each (int) : to print cost each how many iterations
            learning_rate (float) : the learn rate hyper parmeter
            reg_term (float) : the learn rate hyper parmeter
            batch_size (int) : how big is the mini batch and 0 for batch gradint
            optm_func (function) : a function for calling the wanted optmizer
       
        retruns:
            parmeters (dic) : weights and biasses after training
            cost (float) : cost
        &#34;&#34;&#34;

        parameters, costs = opt_func(self, X, Y, num_iterations, print_cost, print_cost_each, cont, learning_rate,reg_term, batch_size, param_dic, drop)
        return parameters, costs

    def predict(self, X):
        &#34;&#34;&#34; perdict classes or output

        parmeters:
            X (np.array) :  input feature vector
        
        retruns:
            Alast (np.array) : output of last layer
        &#34;&#34;&#34;

        Alast, cache = self.forward_propagation(X)
        #predictions = (Alast &gt; thres) * 1

        return Alast

    def test(self, X, Y,eval_func=accuracy_score):
        &#34;&#34;&#34; evalute model

        parmeters:
            X (np.array) :  input feature vector
            Y (np.array) :  the true label
            eval_func (function) : the method of evalution
        
        retruns:
            Alast (np.array) : output of last layer
        &#34;&#34;&#34;


        Alast, cache = self.forward_propagation(X)

        acc = eval_func(Alast,Y)

        return acc</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="Source.frame.MultiLayer"><code class="flex name class">
<span>class <span class="ident">MultiLayer</span></span>
<span>(</span><span>number_of_neurons=0, cost_func=&lt;function cross_entropy&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>init of the class multilayer and needed variables
variables:
w,b lists for weights
parameters dic for weights in the form of parameters['W1']
layers_size for size of each layer
number_of_input_neurons
act_func list for activations of each layer
derivative_act_func list for backward activations derivative functions
cost_func the choosen cost functions</p>
<p>parmeters:
(method) : the cost function of model</p>
<p>returns:
(None)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MultiLayer:
    def __init__(self, number_of_neurons=0, cost_func=cross_entropy):
        &#34;&#34;&#34; init of the class multilayer and needed variables
        variables:
            w,b lists for weights
            parameters dic for weights in the form of parameters[&#39;W1&#39;]
            layers_size for size of each layer
            number_of_input_neurons
            act_func list for activations of each layer
            derivative_act_func list for backward activations derivative functions
            cost_func the choosen cost functions

        parmeters:
            (method) : the cost function of model

        returns:
            (None)

        &#34;&#34;&#34;
        self.w, self.b = [], []
        self.parameters = {}
        self.layer_size = []

        self.number_of_input_neurons = number_of_neurons
        self.number_of_outputs = 0

        self.act_func = []
        self.derivative_act_func = []

        self.cost_func = cost_func
        self.cost_func_der = determine_der_cost_func(self.cost_func)

        self.cache = {}
        self.prev = []

    def addLayerInput(self, size):
        &#34;&#34;&#34; add the input layer of the model

        parmeters:
            size (int) : size of input layer

        retruns:
            (None)

        &#34;&#34;&#34;
        self.number_of_input_neurons = size
        self.layer_size.append(size)

    def addHidenLayer(self, size, act_func=sigmoid):
        &#34;&#34;&#34; add a hidden layer of the model

        parmeters:
            size (int) : size of input layer
            act_func (function) : the activation function of the layer
        
        retruns:
            (None)
        &#34;&#34;&#34;
        self.layer_size.append(size)
        self.act_func.append(act_func)
        self.derivative_act_func.append(determine_der_act_func(act_func))

    def addOutputLayer(self, size, act_func=sigmoid):
        &#34;&#34;&#34; add the output layer of the model

        parmeters:
            size (int) : size of input layer
            act_func (function) : the activation function of the layer
        
        retruns:
            (None)

        &#34;&#34;&#34;
        self.number_of_outputs = size
        self.layer_size.append(size)
        self.act_func.append(act_func)
        self.derivative_act_func.append(determine_der_act_func(act_func))

    def initialize_parameters(self, seed=2): #,init_func=random_init_zero_bias):
        &#34;&#34;&#34; initialize_weights of the model at the start with xavier init

        parmeters:
            seed (int) : seed for random function

        retruns:
            paramters

        &#34;&#34;&#34;

        # todo very important check later

        np.random.seed(seed)  # we set up a seed so that your output matches ours although the initialization is random.

        L = len(self.layer_size)  # number of layers in the network

        for l in range(1, L):
            self.w.append(np.random.randn(self.layer_size[l], self.layer_size[l - 1]) * np.sqrt
            (2 / self.layer_size[l - 1]))  # *0.01
            self.b.append(np.zeros((self.layer_size[l], 1)))
            # seed += 1
            # np.random.seed(seed)

        for i in range(len(self.layer_size) - 1):
            self.parameters[&#34;W&#34; + str(i + 1)] = self.w[i]
            self.parameters[&#34;b&#34; + str(i + 1)] = self.b[i]

        return self.parameters

    def forward_propagation(self, X, drop=0):
        &#34;&#34;&#34; forward propagation through the layers

        parmeters:
            X (np.array) : input feature vector
            drop (float) : propablity to keep neurons or shut down
       
        retruns:
            cashe (dic) : the output of each layer in the form of cashe[&#39;Z1&#39;]
            Alast (np.array) : last layer activations


        &#34;&#34;&#34;

        self.prev = []
        self.prev.append((1, X))
        for i in range(len(self.layer_size) - 1):
            Zi = np.dot(self.w[i], self.prev[i][1]) + self.b[i]
            Ai = self.act_func[i](Zi)
            if drop &gt; 0 and i != len(self.layer_size) - 2:
                D = np.random.rand(Ai.shape[0], Ai.shape[1])
                D = D &lt; drop
                Ai = Ai * D
                Ai = Ai / drop

            self.prev.append((Zi, Ai))

        A_last = self.prev[-1][1]

        for i in range(len(self.layer_size) - 1):
            self.cache[&#34;Z&#34; + str(i + 1)] = self.prev[i + 1][0]
            self.cache[&#34;A&#34; + str(i + 1)] = self.prev[i + 1][1]

        # todo sould i compute cost in here

        return A_last, self.cache

    def set_cost(self, cost_func):
        &#34;&#34;&#34; cahnge the initial cost function

        parmeters:
            cost_funct (function) : the new function
        
        retruns:
            cashe (dic) : the output of each layer in the form of cashe[&#39;Z1&#39;]
            Alast (np.array) : last layer activations

        &#34;&#34;&#34;
        self.cost_func = cost_func
        self.cost_func_der = determine_der_cost_func(cost_func)

    def compute_cost(self, Alast, Y):
        &#34;&#34;&#34; compute cost of the given examples

        parmeters:
            Alast (np.array) : model predictions
            Y (np.array) : True labels
        
        retruns:
            cost (float) : cost output

        &#34;&#34;&#34;
        m = Alast.shape[1]
        return self.cost_func(m, Alast, Y)

    def backward_propagation(self, X, Y):
        &#34;&#34;&#34; compute cost of the given examples

        parmeters:
            Alast (np.array) : model predictions
            Y (np.array) : True labels
        
        retruns:
            grads (dic) : all gridients of wieghts and biasses

        &#34;&#34;&#34;

        m = X.shape[1]

        # todo all depends on the type of function in cost and actviation function
        grad_list1_w = []
        grad_list1_b = []

        Alast = self.prev[-1][1]
        final_act = self.derivative_act_func[-1]
        dzi = self.cost_func_der(m, Alast, Y) * final_act(Alast)

        if self.cost_func == cross_entropy:
            if self.act_func[-1] == sigmoid:
                pass

        for i in range(len(self.w), 0, -1):
            A = self.prev[i - 1][1]
            dwi = (1 / m) * np.dot(dzi, self.prev[i - 1][1].T)
            dbi = (1 / m) * np.sum(dzi, axis=1, keepdims=True)
            if i != 1:
                der_func = self.derivative_act_func[i - 2]
                A = self.prev[i - 1][1]
                dzi = np.multiply(np.dot((self.w[i - 1]).T, dzi), der_func(A))

            grad_list1_w.append(dwi)
            grad_list1_b.append(dbi)

        # reverse grad list
        grad_list_w = []
        grad_list_b = []

        for i in range(len(grad_list1_w) - 1, -1, -1):
            grad_list_w.append(grad_list1_w[i])
            grad_list_b.append(grad_list1_b[i])

        grads = {}

        for i in range(len(grad_list_w)):
            grads[&#39;dW&#39; + str(i + 1)] = grad_list_w[i]
            grads[&#39;db&#39; + str(i + 1)] = grad_list_b[i]

        return grads

    def set_cashe(self, cache, X):
        &#34;&#34;&#34; set an external cache

        parmeters:
            X (np.array) : input feature vector
            cache (dic) :  output of each layer
        
        retruns:
            (None)

        &#34;&#34;&#34;
        self.cache = cache
        self.prev = []
        self.prev.append((1, X))
        for i in range(int(len(cache.keys()) / 2)):
            A, Z = cache[&#34;A&#34; + str(i + 1)], cache[&#34;Z&#34; + str(i + 1)]
            self.prev.append((Z, A))

    def set_parameters(self, para):
        &#34;&#34;&#34; set an external parmeters

        parmeters:
            para (dic) :  the weights and biasses
        
        retruns:
            (None)

        &#34;&#34;&#34;
        self.parameters = para
        self.w = []
        self.b = []
        for i in range(int(len(para.keys()) / 2)):
            W, b = para[&#34;W&#34; + str(i + 1)], para[&#34;b&#34; + str(i + 1)]
            self.w.append(W)
            self.b.append(b)

    def set_parameters_internal(self):
        &#34;&#34;&#34; set an internal parmeters this is used by model during training

        parmeters:
            (None)
        
        retruns:
            (None)

        &#34;&#34;&#34;
        self.parameters = {}
        for i in range(len(self.w)):
            self.parameters[&#34;W&#34; + str(i + 1)] = self.w[i]
            self.parameters[&#34;b&#34; + str(i + 1)] = self.b[i]

    def update_parameters(self, grads, learning_rate=1.2 , reg_term=0, m = 1):
        &#34;&#34;&#34; update parameters using grads

        parmeters:
            grads (dic) :  the gradient of weights and biases
            learning_rate (float) : the learn rate hyper parameter
            reg_term (float) : the learn rate hyper parameter
        
        returns:
            dictionary contains the updated parameters

        &#34;&#34;&#34;

        for i in range(len(self.w)):
            self.w[i] = (1-reg_term/m) * self.w[i] - learning_rate * grads[&#34;dW&#34; + str(i + 1)]
            self.b[i] = (1-reg_term/m) * self.b[i] - learning_rate * grads[&#34;db&#34; + str(i + 1)]

        self.set_parameters_internal()

        return self.parameters

    def update_parameters_adagrad(self, grads,adagrads, learning_rate=1.2, reg_term=0, m = 1):
        &#34;&#34;&#34; update parameters using adagrad

        parameters:
            grads (dic) :  the gradient of weights and biases
            adagrads(dic): the square of the gradiant
            learning_rate (float) : the learn rate hyper parameter
            reg_term (float) : the learn rate hyper parameter
        
        returns:
            dictionary contains the updated parameters

        &#34;&#34;&#34;

        for i in range(len(self.w)):

            self.w[i] = (1-reg_term/m) * self.w[i] - (learning_rate / (np.sqrt(adagrads[&#34;dW&#34; + str(i + 1)]) + 0.000000001)) * grads[&#34;dW&#34; + str(i + 1)]
            self.b[i] = (1-reg_term/m) * self.b[i] - (learning_rate / (np.sqrt(adagrads[&#34;db&#34;+str(i+1)]) + 0.000000001)) * grads[&#34;db&#34; + str(i + 1)]
        self.set_parameters_internal()

        return self.parameters

    def upadte_patameters_RMS(self, grads,rmsgrads, learning_rate=1.2 , reg_term=0, m = 1,eps=None):
        &#34;&#34;&#34; update parameters using RMS gradient

        parameters:
            grads (dic) :  the gradient of weights and biases
            rmsgrads(dic): taking rho multiplied by the square of previous grads and (1-rho) multiplied by the square of current grads
            learning_rate (float) : the learn rate hyper parameter
            reg_term (float) : the learn rate hyper parameter
            eps(float) : the small value added to rmsgrads to make sure there is no division by zero
       
        returns:
            dictionary contains the updated parameters

        &#34;&#34;&#34;

        for i in range(len(self.w)):

            self.w[i] = (1-reg_term/m) * self.w[i] - (learning_rate / (np.sqrt(rmsgrads[&#34;dW&#34; + str(i + 1)]) + eps)) * grads[&#34;dW&#34; + str(i + 1)]
            self.b[i] = (1-reg_term/m) * self.b[i] - (learning_rate / (np.sqrt(rmsgrads[&#34;db&#34;+str(i+1)]) + eps)) * grads[&#34;db&#34; + str(i + 1)]
        self.set_parameters_internal()

        return self.parameters

    def upadte_patameters_adadelta(self, grads,delta, learning_rate=1.2, reg_term=0, m = 1):
        &#34;&#34;&#34; update parameters using RMS gradient

        parameters:
            grads (dic) :  the gradient of weights and biases, note: this parameter is not used in this function
            delta(dic): dictionary contains the values that should be subtracted from current parameters to be updated
            learning_rate (float) : the learn rate hyper parameter , note: this parameter is not used in this function
            reg_term (float) : the learn rate hyper parameter
       
        returns:
            dictionary contains the updated parameters

        &#34;&#34;&#34;


        for i in range(len(self.w)):

            self.w[i] = (1-reg_term/m) * self.w[i] - delta[&#34;dW&#34; + str(i + 1)]
            self.b[i] = (1-reg_term/m) * self.b[i] - delta[&#34;db&#34; + str(i + 1)]
        self.set_parameters_internal()

        return self.parameters

    def update_parameters_adam(self, grads,adamgrads,Fgrads, learning_rate=1.2, reg_term=0, m = 1,eps=None):
        &#34;&#34;&#34; update parameters using RMS gradient

        parameters:
            grads (dic) :  the gradient of weights and biases , note: grads is not used in this function
            adamgrads(dic): taking rho multiplied by the square of previous grads and (1-rho) multiplied by the square of current grads
            Fgrads(dic): taking rhof multiplied by the  previous grads and (1-rhof) multiplied by the  current grads
            learning_rate (float) : the learn rate hyper parameter (alpha_t not alpha)
            reg_term (float) : the learn rate hyper parameter
            eps(float) : the small value added to adamgrads to make sure there is no division by zero
        
        returns:
            dictionary contains the updated parameters

        &#34;&#34;&#34;

        for i in range(len(self.w)):

            self.w[i] = (1-reg_term/m) * self.w[i] - (learning_rate/np.sqrt(adamgrads[&#34;dW&#34;+str(i+1)] + eps)) * Fgrads[&#34;dW&#34; + str(i + 1)]
            self.b[i] = (1-reg_term/m) * self.b[i] - (learning_rate /np.sqrt(adamgrads[&#34;db&#34;+str(i+1)] + eps)) * Fgrads[&#34;db&#34; + str(i + 1)]
        self.set_parameters_internal()

        return self.parameters

    def train(self, X, Y, num_iterations=10000, print_cost=False , print_cost_each=100, cont=0, learning_rate=1 , reg_term=0 , batch_size=0 , opt_func=gd_optm, param_dic=None,drop=0):
        &#34;&#34;&#34; train giving the data and hpyerparmeters and optmizer type
        
        parmeters:
            X (np.array) : input feature vector
            Y (np.array) :  the true label
            num_of iterations (int) : how many epochs
            print cost (bool) : to print cost or not
            print cost_each (int) : to print cost each how many iterations
            learning_rate (float) : the learn rate hyper parmeter
            reg_term (float) : the learn rate hyper parmeter
            batch_size (int) : how big is the mini batch and 0 for batch gradint
            optm_func (function) : a function for calling the wanted optmizer
       
        retruns:
            parmeters (dic) : weights and biasses after training
            cost (float) : cost
        &#34;&#34;&#34;

        parameters, costs = opt_func(self, X, Y, num_iterations, print_cost, print_cost_each, cont, learning_rate,reg_term, batch_size, param_dic, drop)
        return parameters, costs

    def predict(self, X):
        &#34;&#34;&#34; perdict classes or output

        parmeters:
            X (np.array) :  input feature vector
        
        retruns:
            Alast (np.array) : output of last layer
        &#34;&#34;&#34;

        Alast, cache = self.forward_propagation(X)
        #predictions = (Alast &gt; thres) * 1

        return Alast

    def test(self, X, Y,eval_func=accuracy_score):
        &#34;&#34;&#34; evalute model

        parmeters:
            X (np.array) :  input feature vector
            Y (np.array) :  the true label
            eval_func (function) : the method of evalution
        
        retruns:
            Alast (np.array) : output of last layer
        &#34;&#34;&#34;


        Alast, cache = self.forward_propagation(X)

        acc = eval_func(Alast,Y)

        return acc</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Source.frame.MultiLayer.addHidenLayer"><code class="name flex">
<span>def <span class="ident">addHidenLayer</span></span>(<span>self, size, act_func=&lt;function sigmoid&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>add a hidden layer of the model</p>
<p>parmeters:
size (int) : size of input layer
act_func (function) : the activation function of the layer</p>
<p>retruns:
(None)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def addHidenLayer(self, size, act_func=sigmoid):
    &#34;&#34;&#34; add a hidden layer of the model

    parmeters:
        size (int) : size of input layer
        act_func (function) : the activation function of the layer
    
    retruns:
        (None)
    &#34;&#34;&#34;
    self.layer_size.append(size)
    self.act_func.append(act_func)
    self.derivative_act_func.append(determine_der_act_func(act_func))</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.addLayerInput"><code class="name flex">
<span>def <span class="ident">addLayerInput</span></span>(<span>self, size)</span>
</code></dt>
<dd>
<div class="desc"><p>add the input layer of the model</p>
<p>parmeters:
size (int) : size of input layer</p>
<p>retruns:
(None)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def addLayerInput(self, size):
    &#34;&#34;&#34; add the input layer of the model

    parmeters:
        size (int) : size of input layer

    retruns:
        (None)

    &#34;&#34;&#34;
    self.number_of_input_neurons = size
    self.layer_size.append(size)</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.addOutputLayer"><code class="name flex">
<span>def <span class="ident">addOutputLayer</span></span>(<span>self, size, act_func=&lt;function sigmoid&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>add the output layer of the model</p>
<p>parmeters:
size (int) : size of input layer
act_func (function) : the activation function of the layer</p>
<p>retruns:
(None)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def addOutputLayer(self, size, act_func=sigmoid):
    &#34;&#34;&#34; add the output layer of the model

    parmeters:
        size (int) : size of input layer
        act_func (function) : the activation function of the layer
    
    retruns:
        (None)

    &#34;&#34;&#34;
    self.number_of_outputs = size
    self.layer_size.append(size)
    self.act_func.append(act_func)
    self.derivative_act_func.append(determine_der_act_func(act_func))</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.backward_propagation"><code class="name flex">
<span>def <span class="ident">backward_propagation</span></span>(<span>self, X, Y)</span>
</code></dt>
<dd>
<div class="desc"><p>compute cost of the given examples</p>
<p>parmeters:
Alast (np.array) : model predictions
Y (np.array) : True labels</p>
<p>retruns:
grads (dic) : all gridients of wieghts and biasses</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backward_propagation(self, X, Y):
    &#34;&#34;&#34; compute cost of the given examples

    parmeters:
        Alast (np.array) : model predictions
        Y (np.array) : True labels
    
    retruns:
        grads (dic) : all gridients of wieghts and biasses

    &#34;&#34;&#34;

    m = X.shape[1]

    # todo all depends on the type of function in cost and actviation function
    grad_list1_w = []
    grad_list1_b = []

    Alast = self.prev[-1][1]
    final_act = self.derivative_act_func[-1]
    dzi = self.cost_func_der(m, Alast, Y) * final_act(Alast)

    if self.cost_func == cross_entropy:
        if self.act_func[-1] == sigmoid:
            pass

    for i in range(len(self.w), 0, -1):
        A = self.prev[i - 1][1]
        dwi = (1 / m) * np.dot(dzi, self.prev[i - 1][1].T)
        dbi = (1 / m) * np.sum(dzi, axis=1, keepdims=True)
        if i != 1:
            der_func = self.derivative_act_func[i - 2]
            A = self.prev[i - 1][1]
            dzi = np.multiply(np.dot((self.w[i - 1]).T, dzi), der_func(A))

        grad_list1_w.append(dwi)
        grad_list1_b.append(dbi)

    # reverse grad list
    grad_list_w = []
    grad_list_b = []

    for i in range(len(grad_list1_w) - 1, -1, -1):
        grad_list_w.append(grad_list1_w[i])
        grad_list_b.append(grad_list1_b[i])

    grads = {}

    for i in range(len(grad_list_w)):
        grads[&#39;dW&#39; + str(i + 1)] = grad_list_w[i]
        grads[&#39;db&#39; + str(i + 1)] = grad_list_b[i]

    return grads</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.compute_cost"><code class="name flex">
<span>def <span class="ident">compute_cost</span></span>(<span>self, Alast, Y)</span>
</code></dt>
<dd>
<div class="desc"><p>compute cost of the given examples</p>
<p>parmeters:
Alast (np.array) : model predictions
Y (np.array) : True labels</p>
<p>retruns:
cost (float) : cost output</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_cost(self, Alast, Y):
    &#34;&#34;&#34; compute cost of the given examples

    parmeters:
        Alast (np.array) : model predictions
        Y (np.array) : True labels
    
    retruns:
        cost (float) : cost output

    &#34;&#34;&#34;
    m = Alast.shape[1]
    return self.cost_func(m, Alast, Y)</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.forward_propagation"><code class="name flex">
<span>def <span class="ident">forward_propagation</span></span>(<span>self, X, drop=0)</span>
</code></dt>
<dd>
<div class="desc"><p>forward propagation through the layers</p>
<p>parmeters:
X (np.array) : input feature vector
drop (float) : propablity to keep neurons or shut down</p>
<p>retruns:
cashe (dic) : the output of each layer in the form of cashe['Z1']
Alast (np.array) : last layer activations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_propagation(self, X, drop=0):
    &#34;&#34;&#34; forward propagation through the layers

    parmeters:
        X (np.array) : input feature vector
        drop (float) : propablity to keep neurons or shut down
   
    retruns:
        cashe (dic) : the output of each layer in the form of cashe[&#39;Z1&#39;]
        Alast (np.array) : last layer activations


    &#34;&#34;&#34;

    self.prev = []
    self.prev.append((1, X))
    for i in range(len(self.layer_size) - 1):
        Zi = np.dot(self.w[i], self.prev[i][1]) + self.b[i]
        Ai = self.act_func[i](Zi)
        if drop &gt; 0 and i != len(self.layer_size) - 2:
            D = np.random.rand(Ai.shape[0], Ai.shape[1])
            D = D &lt; drop
            Ai = Ai * D
            Ai = Ai / drop

        self.prev.append((Zi, Ai))

    A_last = self.prev[-1][1]

    for i in range(len(self.layer_size) - 1):
        self.cache[&#34;Z&#34; + str(i + 1)] = self.prev[i + 1][0]
        self.cache[&#34;A&#34; + str(i + 1)] = self.prev[i + 1][1]

    # todo sould i compute cost in here

    return A_last, self.cache</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.initialize_parameters"><code class="name flex">
<span>def <span class="ident">initialize_parameters</span></span>(<span>self, seed=2)</span>
</code></dt>
<dd>
<div class="desc"><p>initialize_weights of the model at the start with xavier init</p>
<p>parmeters:
seed (int) : seed for random function</p>
<p>retruns:
paramters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_parameters(self, seed=2): #,init_func=random_init_zero_bias):
    &#34;&#34;&#34; initialize_weights of the model at the start with xavier init

    parmeters:
        seed (int) : seed for random function

    retruns:
        paramters

    &#34;&#34;&#34;

    # todo very important check later

    np.random.seed(seed)  # we set up a seed so that your output matches ours although the initialization is random.

    L = len(self.layer_size)  # number of layers in the network

    for l in range(1, L):
        self.w.append(np.random.randn(self.layer_size[l], self.layer_size[l - 1]) * np.sqrt
        (2 / self.layer_size[l - 1]))  # *0.01
        self.b.append(np.zeros((self.layer_size[l], 1)))
        # seed += 1
        # np.random.seed(seed)

    for i in range(len(self.layer_size) - 1):
        self.parameters[&#34;W&#34; + str(i + 1)] = self.w[i]
        self.parameters[&#34;b&#34; + str(i + 1)] = self.b[i]

    return self.parameters</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>perdict classes or output</p>
<p>parmeters:
X (np.array) :
input feature vector</p>
<p>retruns:
Alast (np.array) : output of last layer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X):
    &#34;&#34;&#34; perdict classes or output

    parmeters:
        X (np.array) :  input feature vector
    
    retruns:
        Alast (np.array) : output of last layer
    &#34;&#34;&#34;

    Alast, cache = self.forward_propagation(X)
    #predictions = (Alast &gt; thres) * 1

    return Alast</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.set_cashe"><code class="name flex">
<span>def <span class="ident">set_cashe</span></span>(<span>self, cache, X)</span>
</code></dt>
<dd>
<div class="desc"><p>set an external cache</p>
<p>parmeters:
X (np.array) : input feature vector
cache (dic) :
output of each layer</p>
<p>retruns:
(None)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_cashe(self, cache, X):
    &#34;&#34;&#34; set an external cache

    parmeters:
        X (np.array) : input feature vector
        cache (dic) :  output of each layer
    
    retruns:
        (None)

    &#34;&#34;&#34;
    self.cache = cache
    self.prev = []
    self.prev.append((1, X))
    for i in range(int(len(cache.keys()) / 2)):
        A, Z = cache[&#34;A&#34; + str(i + 1)], cache[&#34;Z&#34; + str(i + 1)]
        self.prev.append((Z, A))</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.set_cost"><code class="name flex">
<span>def <span class="ident">set_cost</span></span>(<span>self, cost_func)</span>
</code></dt>
<dd>
<div class="desc"><p>cahnge the initial cost function</p>
<p>parmeters:
cost_funct (function) : the new function</p>
<p>retruns:
cashe (dic) : the output of each layer in the form of cashe['Z1']
Alast (np.array) : last layer activations</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_cost(self, cost_func):
    &#34;&#34;&#34; cahnge the initial cost function

    parmeters:
        cost_funct (function) : the new function
    
    retruns:
        cashe (dic) : the output of each layer in the form of cashe[&#39;Z1&#39;]
        Alast (np.array) : last layer activations

    &#34;&#34;&#34;
    self.cost_func = cost_func
    self.cost_func_der = determine_der_cost_func(cost_func)</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.set_parameters"><code class="name flex">
<span>def <span class="ident">set_parameters</span></span>(<span>self, para)</span>
</code></dt>
<dd>
<div class="desc"><p>set an external parmeters</p>
<p>parmeters:
para (dic) :
the weights and biasses</p>
<p>retruns:
(None)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_parameters(self, para):
    &#34;&#34;&#34; set an external parmeters

    parmeters:
        para (dic) :  the weights and biasses
    
    retruns:
        (None)

    &#34;&#34;&#34;
    self.parameters = para
    self.w = []
    self.b = []
    for i in range(int(len(para.keys()) / 2)):
        W, b = para[&#34;W&#34; + str(i + 1)], para[&#34;b&#34; + str(i + 1)]
        self.w.append(W)
        self.b.append(b)</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.set_parameters_internal"><code class="name flex">
<span>def <span class="ident">set_parameters_internal</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>set an internal parmeters this is used by model during training</p>
<p>parmeters:
(None)</p>
<p>retruns:
(None)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_parameters_internal(self):
    &#34;&#34;&#34; set an internal parmeters this is used by model during training

    parmeters:
        (None)
    
    retruns:
        (None)

    &#34;&#34;&#34;
    self.parameters = {}
    for i in range(len(self.w)):
        self.parameters[&#34;W&#34; + str(i + 1)] = self.w[i]
        self.parameters[&#34;b&#34; + str(i + 1)] = self.b[i]</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.test"><code class="name flex">
<span>def <span class="ident">test</span></span>(<span>self, X, Y, eval_func=&lt;function accuracy_score&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>evalute model</p>
<p>parmeters:
X (np.array) :
input feature vector
Y (np.array) :
the true label
eval_func (function) : the method of evalution</p>
<p>retruns:
Alast (np.array) : output of last layer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def test(self, X, Y,eval_func=accuracy_score):
    &#34;&#34;&#34; evalute model

    parmeters:
        X (np.array) :  input feature vector
        Y (np.array) :  the true label
        eval_func (function) : the method of evalution
    
    retruns:
        Alast (np.array) : output of last layer
    &#34;&#34;&#34;


    Alast, cache = self.forward_propagation(X)

    acc = eval_func(Alast,Y)

    return acc</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, X, Y, num_iterations=10000, print_cost=False, print_cost_each=100, cont=0, learning_rate=1, reg_term=0, batch_size=0, opt_func=&lt;function gd_optm&gt;, param_dic=None, drop=0)</span>
</code></dt>
<dd>
<div class="desc"><p>train giving the data and hpyerparmeters and optmizer type</p>
<p>parmeters:
X (np.array) : input feature vector
Y (np.array) :
the true label
num_of iterations (int) : how many epochs
print cost (bool) : to print cost or not
print cost_each (int) : to print cost each how many iterations
learning_rate (float) : the learn rate hyper parmeter
reg_term (float) : the learn rate hyper parmeter
batch_size (int) : how big is the mini batch and 0 for batch gradint
optm_func (function) : a function for calling the wanted optmizer</p>
<p>retruns:
parmeters (dic) : weights and biasses after training
cost (float) : cost</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, X, Y, num_iterations=10000, print_cost=False , print_cost_each=100, cont=0, learning_rate=1 , reg_term=0 , batch_size=0 , opt_func=gd_optm, param_dic=None,drop=0):
    &#34;&#34;&#34; train giving the data and hpyerparmeters and optmizer type
    
    parmeters:
        X (np.array) : input feature vector
        Y (np.array) :  the true label
        num_of iterations (int) : how many epochs
        print cost (bool) : to print cost or not
        print cost_each (int) : to print cost each how many iterations
        learning_rate (float) : the learn rate hyper parmeter
        reg_term (float) : the learn rate hyper parmeter
        batch_size (int) : how big is the mini batch and 0 for batch gradint
        optm_func (function) : a function for calling the wanted optmizer
   
    retruns:
        parmeters (dic) : weights and biasses after training
        cost (float) : cost
    &#34;&#34;&#34;

    parameters, costs = opt_func(self, X, Y, num_iterations, print_cost, print_cost_each, cont, learning_rate,reg_term, batch_size, param_dic, drop)
    return parameters, costs</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.upadte_patameters_RMS"><code class="name flex">
<span>def <span class="ident">upadte_patameters_RMS</span></span>(<span>self, grads, rmsgrads, learning_rate=1.2, reg_term=0, m=1, eps=None)</span>
</code></dt>
<dd>
<div class="desc"><p>update parameters using RMS gradient</p>
<p>parameters:
grads (dic) :
the gradient of weights and biases
rmsgrads(dic): taking rho multiplied by the square of previous grads and (1-rho) multiplied by the square of current grads
learning_rate (float) : the learn rate hyper parameter
reg_term (float) : the learn rate hyper parameter
eps(float) : the small value added to rmsgrads to make sure there is no division by zero</p>
<p>returns:
dictionary contains the updated parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upadte_patameters_RMS(self, grads,rmsgrads, learning_rate=1.2 , reg_term=0, m = 1,eps=None):
    &#34;&#34;&#34; update parameters using RMS gradient

    parameters:
        grads (dic) :  the gradient of weights and biases
        rmsgrads(dic): taking rho multiplied by the square of previous grads and (1-rho) multiplied by the square of current grads
        learning_rate (float) : the learn rate hyper parameter
        reg_term (float) : the learn rate hyper parameter
        eps(float) : the small value added to rmsgrads to make sure there is no division by zero
   
    returns:
        dictionary contains the updated parameters

    &#34;&#34;&#34;

    for i in range(len(self.w)):

        self.w[i] = (1-reg_term/m) * self.w[i] - (learning_rate / (np.sqrt(rmsgrads[&#34;dW&#34; + str(i + 1)]) + eps)) * grads[&#34;dW&#34; + str(i + 1)]
        self.b[i] = (1-reg_term/m) * self.b[i] - (learning_rate / (np.sqrt(rmsgrads[&#34;db&#34;+str(i+1)]) + eps)) * grads[&#34;db&#34; + str(i + 1)]
    self.set_parameters_internal()

    return self.parameters</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.upadte_patameters_adadelta"><code class="name flex">
<span>def <span class="ident">upadte_patameters_adadelta</span></span>(<span>self, grads, delta, learning_rate=1.2, reg_term=0, m=1)</span>
</code></dt>
<dd>
<div class="desc"><p>update parameters using RMS gradient</p>
<p>parameters:
grads (dic) :
the gradient of weights and biases, note: this parameter is not used in this function
delta(dic): dictionary contains the values that should be subtracted from current parameters to be updated
learning_rate (float) : the learn rate hyper parameter , note: this parameter is not used in this function
reg_term (float) : the learn rate hyper parameter</p>
<p>returns:
dictionary contains the updated parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upadte_patameters_adadelta(self, grads,delta, learning_rate=1.2, reg_term=0, m = 1):
    &#34;&#34;&#34; update parameters using RMS gradient

    parameters:
        grads (dic) :  the gradient of weights and biases, note: this parameter is not used in this function
        delta(dic): dictionary contains the values that should be subtracted from current parameters to be updated
        learning_rate (float) : the learn rate hyper parameter , note: this parameter is not used in this function
        reg_term (float) : the learn rate hyper parameter
   
    returns:
        dictionary contains the updated parameters

    &#34;&#34;&#34;


    for i in range(len(self.w)):

        self.w[i] = (1-reg_term/m) * self.w[i] - delta[&#34;dW&#34; + str(i + 1)]
        self.b[i] = (1-reg_term/m) * self.b[i] - delta[&#34;db&#34; + str(i + 1)]
    self.set_parameters_internal()

    return self.parameters</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.update_parameters"><code class="name flex">
<span>def <span class="ident">update_parameters</span></span>(<span>self, grads, learning_rate=1.2, reg_term=0, m=1)</span>
</code></dt>
<dd>
<div class="desc"><p>update parameters using grads</p>
<p>parmeters:
grads (dic) :
the gradient of weights and biases
learning_rate (float) : the learn rate hyper parameter
reg_term (float) : the learn rate hyper parameter</p>
<p>returns:
dictionary contains the updated parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters(self, grads, learning_rate=1.2 , reg_term=0, m = 1):
    &#34;&#34;&#34; update parameters using grads

    parmeters:
        grads (dic) :  the gradient of weights and biases
        learning_rate (float) : the learn rate hyper parameter
        reg_term (float) : the learn rate hyper parameter
    
    returns:
        dictionary contains the updated parameters

    &#34;&#34;&#34;

    for i in range(len(self.w)):
        self.w[i] = (1-reg_term/m) * self.w[i] - learning_rate * grads[&#34;dW&#34; + str(i + 1)]
        self.b[i] = (1-reg_term/m) * self.b[i] - learning_rate * grads[&#34;db&#34; + str(i + 1)]

    self.set_parameters_internal()

    return self.parameters</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.update_parameters_adagrad"><code class="name flex">
<span>def <span class="ident">update_parameters_adagrad</span></span>(<span>self, grads, adagrads, learning_rate=1.2, reg_term=0, m=1)</span>
</code></dt>
<dd>
<div class="desc"><p>update parameters using adagrad</p>
<p>parameters:
grads (dic) :
the gradient of weights and biases
adagrads(dic): the square of the gradiant
learning_rate (float) : the learn rate hyper parameter
reg_term (float) : the learn rate hyper parameter</p>
<p>returns:
dictionary contains the updated parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_adagrad(self, grads,adagrads, learning_rate=1.2, reg_term=0, m = 1):
    &#34;&#34;&#34; update parameters using adagrad

    parameters:
        grads (dic) :  the gradient of weights and biases
        adagrads(dic): the square of the gradiant
        learning_rate (float) : the learn rate hyper parameter
        reg_term (float) : the learn rate hyper parameter
    
    returns:
        dictionary contains the updated parameters

    &#34;&#34;&#34;

    for i in range(len(self.w)):

        self.w[i] = (1-reg_term/m) * self.w[i] - (learning_rate / (np.sqrt(adagrads[&#34;dW&#34; + str(i + 1)]) + 0.000000001)) * grads[&#34;dW&#34; + str(i + 1)]
        self.b[i] = (1-reg_term/m) * self.b[i] - (learning_rate / (np.sqrt(adagrads[&#34;db&#34;+str(i+1)]) + 0.000000001)) * grads[&#34;db&#34; + str(i + 1)]
    self.set_parameters_internal()

    return self.parameters</code></pre>
</details>
</dd>
<dt id="Source.frame.MultiLayer.update_parameters_adam"><code class="name flex">
<span>def <span class="ident">update_parameters_adam</span></span>(<span>self, grads, adamgrads, Fgrads, learning_rate=1.2, reg_term=0, m=1, eps=None)</span>
</code></dt>
<dd>
<div class="desc"><p>update parameters using RMS gradient</p>
<p>parameters:
grads (dic) :
the gradient of weights and biases , note: grads is not used in this function
adamgrads(dic): taking rho multiplied by the square of previous grads and (1-rho) multiplied by the square of current grads
Fgrads(dic): taking rhof multiplied by the
previous grads and (1-rhof) multiplied by the
current grads
learning_rate (float) : the learn rate hyper parameter (alpha_t not alpha)
reg_term (float) : the learn rate hyper parameter
eps(float) : the small value added to adamgrads to make sure there is no division by zero</p>
<p>returns:
dictionary contains the updated parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_adam(self, grads,adamgrads,Fgrads, learning_rate=1.2, reg_term=0, m = 1,eps=None):
    &#34;&#34;&#34; update parameters using RMS gradient

    parameters:
        grads (dic) :  the gradient of weights and biases , note: grads is not used in this function
        adamgrads(dic): taking rho multiplied by the square of previous grads and (1-rho) multiplied by the square of current grads
        Fgrads(dic): taking rhof multiplied by the  previous grads and (1-rhof) multiplied by the  current grads
        learning_rate (float) : the learn rate hyper parameter (alpha_t not alpha)
        reg_term (float) : the learn rate hyper parameter
        eps(float) : the small value added to adamgrads to make sure there is no division by zero
    
    returns:
        dictionary contains the updated parameters

    &#34;&#34;&#34;

    for i in range(len(self.w)):

        self.w[i] = (1-reg_term/m) * self.w[i] - (learning_rate/np.sqrt(adamgrads[&#34;dW&#34;+str(i+1)] + eps)) * Fgrads[&#34;dW&#34; + str(i + 1)]
        self.b[i] = (1-reg_term/m) * self.b[i] - (learning_rate /np.sqrt(adamgrads[&#34;db&#34;+str(i+1)] + eps)) * Fgrads[&#34;db&#34; + str(i + 1)]
    self.set_parameters_internal()

    return self.parameters</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="Source" href="index.html">Source</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="Source.frame.MultiLayer" href="#Source.frame.MultiLayer">MultiLayer</a></code></h4>
<ul class="">
<li><code><a title="Source.frame.MultiLayer.addHidenLayer" href="#Source.frame.MultiLayer.addHidenLayer">addHidenLayer</a></code></li>
<li><code><a title="Source.frame.MultiLayer.addLayerInput" href="#Source.frame.MultiLayer.addLayerInput">addLayerInput</a></code></li>
<li><code><a title="Source.frame.MultiLayer.addOutputLayer" href="#Source.frame.MultiLayer.addOutputLayer">addOutputLayer</a></code></li>
<li><code><a title="Source.frame.MultiLayer.backward_propagation" href="#Source.frame.MultiLayer.backward_propagation">backward_propagation</a></code></li>
<li><code><a title="Source.frame.MultiLayer.compute_cost" href="#Source.frame.MultiLayer.compute_cost">compute_cost</a></code></li>
<li><code><a title="Source.frame.MultiLayer.forward_propagation" href="#Source.frame.MultiLayer.forward_propagation">forward_propagation</a></code></li>
<li><code><a title="Source.frame.MultiLayer.initialize_parameters" href="#Source.frame.MultiLayer.initialize_parameters">initialize_parameters</a></code></li>
<li><code><a title="Source.frame.MultiLayer.predict" href="#Source.frame.MultiLayer.predict">predict</a></code></li>
<li><code><a title="Source.frame.MultiLayer.set_cashe" href="#Source.frame.MultiLayer.set_cashe">set_cashe</a></code></li>
<li><code><a title="Source.frame.MultiLayer.set_cost" href="#Source.frame.MultiLayer.set_cost">set_cost</a></code></li>
<li><code><a title="Source.frame.MultiLayer.set_parameters" href="#Source.frame.MultiLayer.set_parameters">set_parameters</a></code></li>
<li><code><a title="Source.frame.MultiLayer.set_parameters_internal" href="#Source.frame.MultiLayer.set_parameters_internal">set_parameters_internal</a></code></li>
<li><code><a title="Source.frame.MultiLayer.test" href="#Source.frame.MultiLayer.test">test</a></code></li>
<li><code><a title="Source.frame.MultiLayer.train" href="#Source.frame.MultiLayer.train">train</a></code></li>
<li><code><a title="Source.frame.MultiLayer.upadte_patameters_RMS" href="#Source.frame.MultiLayer.upadte_patameters_RMS">upadte_patameters_RMS</a></code></li>
<li><code><a title="Source.frame.MultiLayer.upadte_patameters_adadelta" href="#Source.frame.MultiLayer.upadte_patameters_adadelta">upadte_patameters_adadelta</a></code></li>
<li><code><a title="Source.frame.MultiLayer.update_parameters" href="#Source.frame.MultiLayer.update_parameters">update_parameters</a></code></li>
<li><code><a title="Source.frame.MultiLayer.update_parameters_adagrad" href="#Source.frame.MultiLayer.update_parameters_adagrad">update_parameters_adagrad</a></code></li>
<li><code><a title="Source.frame.MultiLayer.update_parameters_adam" href="#Source.frame.MultiLayer.update_parameters_adam">update_parameters_adam</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>